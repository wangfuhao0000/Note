## 发现问题

下边是一个订单表的简单表结构。里边有大概270万条数据，其中渠道ID为35的有132万调数据。

```mysql
CREATE TABLE IF NOT EXISTS `order_info` (
  `order_id` int(11) NOT NULL AUTO_INCREMENT COMMENT '订单ID',
  `order_sn` varchar(60) NOT NULL COMMENT '订单号',
  `user_id` int(11) NOT NULL COMMENT '用户ID',
  `channels_id` int(11) NOT NULL COMMENT '渠道ID',
  ……一些其他字段
  `order_time` datetime NOT NULL COMMENT '下单时间',  
  PRIMARY KEY (`order_id`),
  KEY `channels_id` (`channels_id`),
  KEY `order_sn` (`order_sn`),
  KEY `user_id` (`user_id`),
  KEY `order_time` (`order_time`)
) ENGINE=InnoDB  DEFAULT CHARSET=utf8;
```

一个订单列表页面一般很多人是这么写的。显示一个总数或者总页数，然后是上一页 1 2 3 4 5 下一页

![img](https:////upload-images.jianshu.io/upload_images/2795495-a3cf63cdc7e62656?imageMogr2/auto-orient/strip|imageView2/2/w/449/format/webp)

而我们一般会这样写sql语句去实现上边的功能：

```mysql
select count(1) as num from order_info where channels_id=35;    0.24 sec
select * from order_info where channels_id=35 order by order_id desc limit 0,20;    0.01 sec
select * from order_info where channels_id=35 order by order_id desc limit 1320000,20;  12.55 sec  即便是第二次查询也用了4.27 sec（mysql自身也会有查询缓存机制）
```

这里获取数据总数用了相当长的时间。随着你数据量的增多需要的时间也会更长。在获取第一页的数据的时候也没用多长时间，但是越往后需要的时间也就越长。
 在多人操作尤其是大并发量的情况下，大量的数据被扫描造成系统IO和CPU资源消耗完，进而导致整个数据库不可服务。 而cpu 消耗过大通常情况下都是由于慢sql 造成的，这里的慢sql 包括全表扫描，扫描数据量过大，内存排序，磁盘排序，锁争用等待等； 表现现象为：sql 执行状态为：sending data，Copying to tmp table，Copying to tmp table on disk，Sorting result，locked;

## 如何优化

**普通的limit M,N 的写法越往后查询越慢。因为mysql总是会去扫描M+N条数据来得到你想要的数据。**

我们来看一下京东的分页



![img](https:////upload-images.jianshu.io/upload_images/2795495-10c16932e9a89504?imageMogr2/auto-orient/strip|imageView2/2/w/1106/format/webp)



![img](https:////upload-images.jianshu.io/upload_images/2795495-d877503a84956d89?imageMogr2/auto-orient/strip|imageView2/2/w/391/format/webp)



上边是京东的搜索和分页。京东的订单很明显**根据时间维度做了分库或者分表**，也可能根据用户维度又做了分库分表。京东没有显示总数，但是显示了页码 1 2 3 4 5

### 获取数据总数的优化

**尽量不要去获取数据总数**。如果业务确实需要获取当前搜索条件下的数据总数也建议使用ajax让用户点击按钮触发后获取总数，或者根据时间维度做数据的分表。大多数用户在点击订单列表的时候关心的不是订单总数，也不是很久之前的订单，而是最近一段时间下的订单。

### 获取数据的优化(先查主键，再根据主键查数据)

下边我们利用索引只获取主键ID。用了0.40 sec，比上边的sql少了很多。

```mysql
select order_id from order_info where channels_id=35 order by order_id desc limit 1320000,20;   0.40 sec
```

所以我们可以有这样的优化写法：

```mysql
select * from order_info,(select order_id from order_info where channels_id=35 order by order_id desc limit 1320000,20) order_info_tmp where order_info.order_id = order_info_tmp.order_id; 0.47 sec

select * from order_info,(select order_id from order_info where channels_id=35 order by order_id desc limit 0,20) order_info_tmp where order_info.order_id = order_info_tmp.order_id; 0.00 sec
```

**先查询翻页中需要的N条数据的主键id，然后根据主键id去查询你所需要的N条数据，此过程中查询N条数据的主键ID在索引中完成。**

这里我们尽量只显示上一页或者下一页。那么如何去判断下一页是否有数据呢（没有数据的时候把下一页的按钮置灰）?**参考laravel的简单分页设计。比如每页显示20条数据，而我显示当前页面的时候去获取21条数据，根据是否存在第21条数据来判断是否需要显示下一页。**
 如果需要显示页码1 2 3 4 5呢？其实也可以获取当前范围的几个页面的数据来判断，尽量减少扫描范围。

------

上边的方法虽然快了不少，可是依然扫描了很多的数据行，在数据量大的情况下依然会很慢，尤其是在做数据导出的时候。
 比较常见的导出数据的应用场景就是用户输入搜索条件然后按照搜索条件导出数据。数据的导出不像列表页的显示。我们完全可以利用主键来操作。

```csharp
select * from order_info where channels_id=35 AND order_id <=54388 order by order_id desc limit 20; 0.00 sec
```

我们主要是利用了主键ID，这里你可以看到即便是非常往后的数据也是很快的速度就能获取到。这样写能很大程度上减少表扫描的行数，减少数据查询的时间。

```php
//auth by duxiaokong 2016-08-23
$fp = fopen('php://output', 'a');
$num_limit = 1000;
$order_id = 0;
$order_list = [];
while (true) {
    //执行sql  select * from order_info where $where AND order_id > $order_id order by order_id ASC limit $num_limit; 得到$order_list订单列表
    //这里一定要注意 order_id > $order_id 和 order_id ASC的排序
    if (empty($order_list)) {
        break;
    }
    $line = 0;
    $row_str = '';
    foreach ($order_list as $key => $val) {
        $order_id = $val['order_id']; //这行代码一定要记得赋值不然会造成死循环
        $line++;
        // 获取导出数据
        $row = [
            $val['order_sn'],
            $val['order_time'],
            $val['user_name']
            // ……
        ];
        //$row 过滤 $row中的非法字符
        $row_str .= mb_convert_encoding(implode(',', $row), 'gbk', 'utf-8') . PHP_EOL;
        //每获取20次记录写入一次数据库，减少IO
        if ($line >= 20) {
            fwrite($fp, $row_str);
            $line = 0;
            $row_str = '';
        }
    }
    if (!empty($row_str)) {
        fwrite($fp, $row_str);
        $line = 0;
        $row_str = '';
    }
}
fclose($fp);
```

> **总结：如何优化？最主要的原则就是避免数据量大时扫描过多的记录。**